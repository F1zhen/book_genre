{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Enhanced Book Genre Classification with ML and DL Models\n",
    "\n",
    "This notebook implements a comprehensive comparison of various machine learning and deep learning models for book genre classification, including:\n",
    "- Classic ML models (SVM, Random Forest, Naive Bayes, Logistic Regression)\n",
    "- Neural Networks (MLP, CNN, LSTM, GRU)\n",
    "- Transformer models (BERT, RoBERTa, DistilBERT, XLM-RoBERTa)\n",
    "- PCA visualization and dimensionality reduction\n",
    "- Comprehensive performance analysis with confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, Conv1D, GlobalMaxPooling1D, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, \n",
    "    BertTokenizer, BertModel,\n",
    "    RobertaTokenizer, RobertaModel,\n",
    "    DistilBertTokenizer, DistilBertModel,\n",
    "    XLMRobertaTokenizer, XLMRobertaModel\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Time tracking\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"../data/goodreads_data.csv\")\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clean_data"
   },
   "outputs": [],
   "source": [
    "# Remove unnamed column if exists\n",
    "if 'Unnamed: 0' in data.columns:\n",
    "    data = data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Remove duplicates and null values\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "print(f\"After cleaning: {data.shape}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nData Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "english_filtering"
   },
   "source": [
    "### English Language Filtering\n",
    "Filter out non-English books from descriptions and associated multilabel genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "filter_english"
   },
   "outputs": [],
   "source": [
    "def is_english_text(text, min_english_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Check if text is primarily in English\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean text for language detection\n",
    "        clean_text = re.sub(r'[^a-zA-Z\\s]', ' ', str(text))\n",
    "        clean_text = ' '.join(clean_text.split())\n",
    "        \n",
    "        if len(clean_text) < 10:  # Too short for reliable detection\n",
    "            return False\n",
    "            \n",
    "        detected_lang = detect(clean_text)\n",
    "        return detected_lang == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def filter_english_books(data, min_english_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Filter dataset to keep only English books\n",
    "    \"\"\"\n",
    "    print(\"Filtering English books...\")\n",
    "    \n",
    "    # Filter based on description language\n",
    "    english_mask = data['Description'].apply(lambda x: is_english_text(x, min_english_ratio))\n",
    "    \n",
    "    print(f\"Books before filtering: {len(data)}\")\n",
    "    print(f\"English books detected: {english_mask.sum()}\")\n",
    "    \n",
    "    # Keep only English books\n",
    "    english_data = data[english_mask].copy()\n",
    "    \n",
    "    print(f\"Books after filtering: {len(english_data)}\")\n",
    "    print(f\"Filtered out: {len(data) - len(english_data)} books\")\n",
    "    \n",
    "    return english_data\n",
    "\n",
    "# Apply English filtering\n",
    "english_data = filter_english_books(data)\n",
    "print(\"\\nSample of filtered data:\")\n",
    "english_data[['Book', 'Author', 'Description']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "genre_processing"
   },
   "source": [
    "### Genre Processing and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process_genres"
   },
   "outputs": [],
   "source": [
    "def extract_genres(genre_string):\n",
    "    \"\"\"\n",
    "    Extract genres from string format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove brackets and split by comma\n",
    "        genres = genre_string.strip(\"[]\").split(\",\")\n",
    "        # Clean each genre\n",
    "        genres = [genre.strip(\" '\") for genre in genres if genre.strip()]\n",
    "        return genres\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Extract all genres\n",
    "english_data['genres_list'] = english_data['Genres'].apply(extract_genres)\n",
    "\n",
    "# Get all unique genres\n",
    "all_genres = set()\n",
    "for genres in english_data['genres_list']:\n",
    "    all_genres.update(genres)\n",
    "\n",
    "print(f\"Total unique genres: {len(all_genres)}\")\n",
    "print(f\"Sample genres: {list(all_genres)[:20]}\")\n",
    "\n",
    "# Count genre frequencies\n",
    "genre_counts = Counter()\n",
    "for genres in english_data['genres_list']:\n",
    "    genre_counts.update(genres)\n",
    "\n",
    "# Get top genres\n",
    "top_genres = dict(genre_counts.most_common(20))\n",
    "print(f\"\\nTop 20 genres:\")\n",
    "for genre, count in top_genres.items():\n",
    "    print(f\"{genre}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_genres"
   },
   "outputs": [],
   "source": [
    "# Visualize genre distribution\n",
    "plt.figure(figsize=(15, 8))\n",
    "genres = list(top_genres.keys())\n",
    "counts = list(top_genres.values())\n",
    "\n",
    "plt.barh(genres, counts)\n",
    "plt.xlabel('Number of Books')\n",
    "plt.title('Top 20 Most Common Genres')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Genre distribution statistics\n",
    "genre_lengths = [len(genres) for genres in english_data['genres_list']]\n",
    "print(f\"\\nGenre statistics:\")\n",
    "print(f\"Average genres per book: {np.mean(genre_lengths):.2f}\")\n",
    "print(f\"Min genres: {min(genre_lengths)}\")\n",
    "print(f\"Max genres: {max(genre_lengths)}\")\n",
    "print(f\"Median genres: {np.median(genre_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "text_preprocessing"
   },
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess_text"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text for better vectorization\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:\"\\'-]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Preprocess descriptions\n",
    "english_data['clean_description'] = english_data['Description'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty descriptions\n",
    "english_data = english_data[english_data['clean_description'].str.len() > 50]\n",
    "\n",
    "print(f\"After text preprocessing: {len(english_data)} books\")\n",
    "print(\"\\nSample preprocessed descriptions:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nBook {i+1}:\")\n",
    "    print(f\"Original: {english_data.iloc[i]['Description'][:200]}...\")\n",
    "    print(f\"Cleaned: {english_data.iloc[i]['clean_description'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multilabel_setup"
   },
   "source": [
    "### Multilabel Classification Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_multilabel"
   },
   "outputs": [],
   "source": [
    "# Select top genres for classification (to avoid too many classes)\n",
    "top_n_genres = 15\n",
    "selected_genres = list(genre_counts.most_common(top_n_genres))\n",
    "selected_genre_names = [genre[0] for genre in selected_genres]\n",
    "\n",
    "print(f\"Selected {top_n_genres} most common genres for classification:\")\n",
    "for i, (genre, count) in enumerate(selected_genres, 1):\n",
    "    print(f\"{i:2d}. {genre:20s} ({count:4d} books)\")\n",
    "\n",
    "# Filter books that have at least one of the selected genres\n",
    "def has_selected_genre(genres_list):\n",
    "    return any(genre in selected_genre_names for genre in genres_list)\n",
    "\n",
    "filtered_data = english_data[english_data['genres_list'].apply(has_selected_genre)].copy()\n",
    "print(f\"\\nBooks with selected genres: {len(filtered_data)}\")\n",
    "\n",
    "# Create multilabel targets\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_multilabel = mlb.fit_transform(filtered_data['genres_list'])\n",
    "\n",
    "# Filter to only include selected genres\n",
    "selected_indices = [mlb.classes_.tolist().index(genre) for genre in selected_genre_names]\n",
    "y_selected = y_multilabel[:, selected_indices]\n",
    "\n",
    "print(f\"\\nMultilabel shape: {y_selected.shape}\")\n",
    "print(f\"Selected genres: {selected_genre_names}\")\n",
    "\n",
    "# Prepare features and targets\n",
    "X_text = filtered_data['clean_description'].values\n",
    "y = y_selected\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"Features shape: {X_text.shape}\")\n",
    "print(f\"Targets shape: {y.shape}\")\n",
    "print(f\"Number of classes: {y.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "classic_ml"
   },
   "source": [
    "## 2. Classic Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "classic_ml_setup"
   },
   "outputs": [],
   "source": [
    "# Split data for classic ML\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=None  # Can't stratify with multilabel\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\nTF-IDF features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "classic_ml_models"
   },
   "outputs": [],
   "source": [
    "# Define classic ML models\n",
    "classic_models = {\n",
    "    'Logistic Regression': OneVsRestClassifier(LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    'Random Forest': OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    'SVM': OneVsRestClassifier(SVC(kernel='linear', random_state=42, probability=True)),\n",
    "    'Naive Bayes': OneVsRestClassifier(MultinomialNB())\n",
    "}\n",
    "\n",
    "# Train and evaluate classic models\n",
    "classic_results = {}\n",
    "\n",
    "for name, model in classic_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    classic_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neural_networks"
   },
   "source": [
    "## 3. Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neural_preprocessing"
   },
   "outputs": [],
   "source": [
    "# Prepare data for neural networks\n",
    "max_words = 10000\n",
    "max_length = 200\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f\"Neural network data shapes:\")\n",
    "print(f\"X_train_padded: {X_train_padded.shape}\")\n",
    "print(f\"X_test_padded: {X_test_padded.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neural_models"
   },
   "outputs": [],
   "source": [
    "def create_mlp_model(input_dim, output_dim, vocab_size, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Create Multi-Layer Perceptron model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=input_dim),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_cnn_model(input_dim, output_dim, vocab_size, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Create Convolutional Neural Network model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=input_dim),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_dim, output_dim, vocab_size, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Create LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=input_dim),\n",
    "        LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_gru_model(input_dim, output_dim, vocab_size, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Create GRU model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=input_dim),\n",
    "        GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        GRU(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define neural network models\n",
    "neural_models = {\n",
    "    'MLP': create_mlp_model,\n",
    "    'CNN': create_cnn_model,\n",
    "    'LSTM': create_lstm_model,\n",
    "    'GRU': create_gru_model\n",
    "}\n",
    "\n",
    "print(\"Neural network model architectures defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_neural_models"
   },
   "outputs": [],
   "source": [
    "# Train neural network models\n",
    "neural_results = {}\n",
    "\n",
    "for name, model_func in neural_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create model\n",
    "    model = model_func(max_length, y_train.shape[1], max_words)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train_padded, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test_padded)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    neural_results[name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.2f}s\")\n",
    "    print(f\"  Epochs: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transformers"
   },
   "source": [
    "## 4. Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transformer_setup"
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define transformer models\n",
    "transformer_configs = {\n",
    "    'BERT': {\n",
    "        'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        'model': BertModel.from_pretrained('bert-base-uncased')\n",
    "    },\n",
    "    'RoBERTa': {\n",
    "        'tokenizer': RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "        'model': RobertaModel.from_pretrained('roberta-base')\n",
    "    },\n",
    "    'DistilBERT': {\n",
    "        'tokenizer': DistilBertTokenizer.from_pretrained('distilbert-base-uncased'),\n",
    "        'model': DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    },\n",
    "    'XLM-RoBERTa': {\n",
    "        'tokenizer': XLMRobertaTokenizer.from_pretrained('xlm-roberta-base'),\n",
    "        'model': XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Transformer models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transformer_dataset"
   },
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(label)\n",
    "        }\n",
    "\n",
    "def extract_features_with_transformer(texts, tokenizer, model, device, batch_size=16):\n",
    "    \"\"\"\n",
    "    Extract features using transformer model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use [CLS] token representation\n",
    "            batch_features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            features.extend(batch_features)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "print(\"Transformer dataset and feature extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_transformers"
   },
   "outputs": [],
   "source": [
    "# Train transformer models\n",
    "transformer_results = {}\n",
    "\n",
    "for name, config in transformer_configs.items():\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokenizer = config['tokenizer']\n",
    "    model = config['model'].to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    print(f\"  Extracting features for {name}...\")\n",
    "    X_train_features = extract_features_with_transformer(X_train, tokenizer, model, device)\n",
    "    X_test_features = extract_features_with_transformer(X_test, tokenizer, model, device)\n",
    "    \n",
    "    print(f\"  Training classifier for {name}...\")\n",
    "    # Train a simple classifier on the features\n",
    "    classifier = OneVsRestClassifier(LogisticRegression(random_state=42, max_iter=1000))\n",
    "    classifier.fit(X_train_features, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test_features)\n",
    "    y_pred_proba = classifier.predict_proba(X_test_features)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    transformer_results[name] = {\n",
    "        'classifier': classifier,\n",
    "        'tokenizer': tokenizer,\n",
    "        'transformer_model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'features': X_test_features\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.2f}s\")\n",
    "    print(f\"  Feature Dimension: {X_train_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pca_visualization"
   },
   "source": [
    "## 5. PCA Visualization and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pca_analysis"
   },
   "outputs": [],
   "source": [
    "# Apply PCA to different feature representations\n",
    "pca_results = {}\n",
    "\n",
    "# 1. TF-IDF features\n",
    "print(\"Applying PCA to TF-IDF features...\")\n",
    "pca_tfidf = PCA(n_components=2)\n",
    "X_tfidf_pca = pca_tfidf.fit_transform(X_train_tfidf.toarray())\n",
    "pca_results['TF-IDF'] = {\n",
    "    'pca': pca_tfidf,\n",
    "    'components': X_tfidf_pca,\n",
    "    'explained_variance_ratio': pca_tfidf.explained_variance_ratio_\n",
    "}\n",
    "\n",
    "# 2. Neural network features (using the best performing model)\n",
    "print(\"Applying PCA to neural network features...\")\n",
    "best_neural_model = max(neural_results.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"Using {best_neural_model[0]} features for PCA\")\n",
    "\n",
    "# Extract features from the best neural model\n",
    "neural_model = best_neural_model[1]['model']\n",
    "feature_extractor = Model(inputs=neural_model.input, outputs=neural_model.layers[-2].output)\n",
    "X_neural_features = feature_extractor.predict(X_train_padded)\n",
    "\n",
    "pca_neural = PCA(n_components=2)\n",
    "X_neural_pca = pca_neural.fit_transform(X_neural_features)\n",
    "pca_results['Neural Network'] = {\n",
    "    'pca': pca_neural,\n",
    "    'components': X_neural_pca,\n",
    "    'explained_variance_ratio': pca_neural.explained_variance_ratio_\n",
    "}\n",
    "\n",
    "# 3. Transformer features (using the best performing transformer)\n",
    "print(\"Applying PCA to transformer features...\")\n",
    "best_transformer = max(transformer_results.items(), key=lambda x: x[1]['f1'])\n",
    "print(f\"Using {best_transformer[0]} features for PCA\")\n",
    "\n",
    "X_transformer_features = best_transformer[1]['features']\n",
    "pca_transformer = PCA(n_components=2)\n",
    "X_transformer_pca = pca_transformer.fit_transform(X_transformer_features)\n",
    "pca_results['Transformer'] = {\n",
    "    'pca': pca_transformer,\n",
    "    'components': X_transformer_pca,\n",
    "    'explained_variance_ratio': pca_transformer.explained_variance_ratio_\n",
    "}\n",
    "\n",
    "print(\"\\nPCA Results:\")\n",
    "for name, result in pca_results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Explained variance ratio: {result['explained_variance_ratio']}\")\n",
    "    print(f\"  Total explained variance: {sum(result['explained_variance_ratio']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pca_visualization_plots"
   },
   "outputs": [],
   "source": [
    "# Create PCA visualization plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Get the most common genre for each sample for coloring\n",
    "def get_primary_genre(genres_list, selected_genres):\n",
    "    for genre in genres_list:\n",
    "        if genre in selected_genres:\n",
    "            return genre\n",
    "    return 'Other'\n",
    "\n",
    "primary_genres = [get_primary_genre(genres, selected_genre_names) for genres in filtered_data['genres_list']]\n",
    "unique_genres = list(set(primary_genres))\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_genres)))\n",
    "genre_color_map = dict(zip(unique_genres, colors))\n",
    "\n",
    "# Plot 1: TF-IDF PCA\n",
    "ax1 = axes[0]\n",
    "for genre in unique_genres:\n",
    "    mask = [g == genre for g in primary_genres]\n",
    "    ax1.scatter(X_tfidf_pca[mask, 0], X_tfidf_pca[mask, 1], \n",
    "               c=[genre_color_map[genre]], label=genre, alpha=0.6, s=20)\n",
    "ax1.set_title('TF-IDF Features PCA')\n",
    "ax1.set_xlabel(f'PC1 ({pca_tfidf.explained_variance_ratio_[0]:.2%})')\n",
    "ax1.set_ylabel(f'PC2 ({pca_tfidf.explained_variance_ratio_[1]:.2%})')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot 2: Neural Network PCA\n",
    "ax2 = axes[1]\n",
    "for genre in unique_genres:\n",
    "    mask = [g == genre for g in primary_genres]\n",
    "    ax2.scatter(X_neural_pca[mask, 0], X_neural_pca[mask, 1], \n",
    "               c=[genre_color_map[genre]], label=genre, alpha=0.6, s=20)\n",
    "ax2.set_title(f'{best_neural_model[0]} Features PCA')\n",
    "ax2.set_xlabel(f'PC1 ({pca_neural.explained_variance_ratio_[0]:.2%})')\n",
    "ax2.set_ylabel(f'PC2 ({pca_neural.explained_variance_ratio_[1]:.2%})')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot 3: Transformer PCA\n",
    "ax3 = axes[2]\n",
    "for genre in unique_genres:\n",
    "    mask = [g == genre for g in primary_genres]\n",
    "    ax3.scatter(X_transformer_pca[mask, 0], X_transformer_pca[mask, 1], \n",
    "               c=[genre_color_map[genre]], label=genre, alpha=0.6, s=20)\n",
    "ax3.set_title(f'{best_transformer[0]} Features PCA')\n",
    "ax3.set_xlabel(f'PC1 ({pca_transformer.explained_variance_ratio_[0]:.2%})')\n",
    "ax3.set_ylabel(f'PC2 ({pca_transformer.explained_variance_ratio_[1]:.2%})')\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create 3D PCA visualization for the best model\n",
    "print(\"\\nCreating 3D PCA visualization...\")\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_3d = pca_3d.fit_transform(X_transformer_features)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for genre in unique_genres:\n",
    "    mask = [g == genre for g in primary_genres]\n",
    "    ax.scatter(X_3d[mask, 0], X_3d[mask, 1], X_3d[mask, 2], \n",
    "              c=[genre_color_map[genre]], label=genre, alpha=0.6, s=20)\n",
    "\n",
    "ax.set_title(f'{best_transformer[0]} Features 3D PCA')\n",
    "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.2%})')\n",
    "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.2%})')\n",
    "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.2%})')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "confusion_matrices"
   },
   "source": [
    "## 6. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion_matrix_plots"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, title, ax=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for multilabel classification\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix for each class\n",
    "    cm_per_class = []\n",
    "    for i in range(len(class_names)):\n",
    "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "        cm_per_class.append(cm)\n",
    "    \n",
    "    # Create subplot for each class\n",
    "    n_classes = len(class_names)\n",
    "    n_cols = min(5, n_classes)\n",
    "    n_rows = (n_classes + n_cols - 1) // n_cols\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 3*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [ax]\n",
    "    \n",
    "    for i, (cm, class_name) in enumerate(zip(cm_per_class, class_names)):\n",
    "        if i < len(axes):\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "            axes[i].set_title(f'{class_name}\\n(TN: {cm[0,0]}, FP: {cm[0,1]}, FN: {cm[1,0]}, TP: {cm[1,1]})')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(class_names), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "# Plot confusion matrices for all models\n",
    "print(\"Creating confusion matrices for all models...\")\n",
    "\n",
    "# Classic ML models\n",
    "for name, result in classic_results.items():\n",
    "    plot_confusion_matrix(\n",
    "        y_test, result['predictions'], \n",
    "        selected_genre_names, \n",
    "        f'{name} Confusion Matrix'\n",
    "    )\n",
    "\n",
    "# Neural network models\n",
    "for name, result in neural_results.items():\n",
    "    plot_confusion_matrix(\n",
    "        y_test, result['predictions'], \n",
    "        selected_genre_names, \n",
    "        f'{name} Confusion Matrix'\n",
    "    )\n",
    "\n",
    "# Transformer models\n",
    "for name, result in transformer_results.items():\n",
    "    plot_confusion_matrix(\n",
    "        y_test, result['predictions'], \n",
    "        selected_genre_names, \n",
    "        f'{name} Confusion Matrix'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_comparison"
   },
   "source": [
    "## 7. Performance Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_summary"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive performance comparison\n",
    "all_results = {}\n",
    "all_results.update(classic_results)\n",
    "all_results.update(neural_results)\n",
    "all_results.update(transformer_results)\n",
    "\n",
    "# Create performance summary dataframe\n",
    "performance_data = []\n",
    "for name, result in all_results.items():\n",
    "    performance_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1-Score': result['f1'],\n",
    "        'Training Time (s)': result['training_time']\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(performance_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1 = axes[0, 0]\n",
    "performance_df_sorted = performance_df.sort_values('Accuracy', ascending=True)\n",
    "ax1.barh(performance_df_sorted['Model'], performance_df_sorted['Accuracy'])\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_xlabel('Accuracy')\n",
    "\n",
    "# F1-Score comparison\n",
    "ax2 = axes[0, 1]\n",
    "performance_df_sorted = performance_df.sort_values('F1-Score', ascending=True)\n",
    "ax2.barh(performance_df_sorted['Model'], performance_df_sorted['F1-Score'])\n",
    "ax2.set_title('Model F1-Score Comparison')\n",
    "ax2.set_xlabel('F1-Score')\n",
    "\n",
    "# Training time comparison\n",
    "ax3 = axes[1, 0]\n",
    "performance_df_sorted = performance_df.sort_values('Training Time (s)', ascending=True)\n",
    "ax3.barh(performance_df_sorted['Model'], performance_df_sorted['Training Time (s)'])\n",
    "ax3.set_title('Model Training Time Comparison')\n",
    "ax3.set_xlabel('Training Time (seconds)')\n",
    "\n",
    "# Precision vs Recall scatter plot\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(performance_df['Precision'], performance_df['Recall'], \n",
    "                     s=100, alpha=0.7, c=performance_df['F1-Score'], cmap='viridis')\n",
    "ax4.set_xlabel('Precision')\n",
    "ax4.set_ylabel('Recall')\n",
    "ax4.set_title('Precision vs Recall (colored by F1-Score)')\n",
    "\n",
    "# Add model labels\n",
    "for i, model in enumerate(performance_df['Model']):\n",
    "    ax4.annotate(model, (performance_df.iloc[i]['Precision'], performance_df.iloc[i]['Recall']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax4, label='F1-Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model category analysis\n",
    "print(\"\\n=== MODEL CATEGORY ANALYSIS ===\")\n",
    "classic_models = [name for name in performance_df['Model'] if name in classic_results]\n",
    "neural_models = [name for name in performance_df['Model'] if name in neural_results]\n",
    "transformer_models = [name for name in performance_df['Model'] if name in transformer_results]\n",
    "\n",
    "print(f\"\\nClassic ML Models (avg F1: {performance_df[performance_df['Model'].isin(classic_models)]['F1-Score'].mean():.4f}):\")\n",
    "for model in classic_models:\n",
    "    f1 = performance_df[performance_df['Model'] == model]['F1-Score'].iloc[0]\n",
    "    print(f\"  {model}: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nNeural Network Models (avg F1: {performance_df[performance_df['Model'].isin(neural_models)]['F1-Score'].mean():.4f}):\")\n",
    "for model in neural_models:\n",
    "    f1 = performance_df[performance_df['Model'] == model]['F1-Score'].iloc[0]\n",
    "    print(f\"  {model}: {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nTransformer Models (avg F1: {performance_df[performance_df['Model'].isin(transformer_models)]['F1-Score'].mean():.4f}):\")\n",
    "for model in transformer_models:\n",
    "    f1 = performance_df[performance_df['Model'] == model]['F1-Score'].iloc[0]\n",
    "    print(f\"  {model}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "detailed_analysis"
   },
   "source": [
    "## 8. Detailed Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed_analysis_code"
   },
   "outputs": [],
   "source": [
    "# Best model analysis\n",
    "best_model_name = performance_df.iloc[0]['Model']\n",
    "best_model_result = all_results[best_model_name]\n",
    "\n",
    "print(f\"\\n=== BEST MODEL ANALYSIS: {best_model_name} ===\")\n",
    "print(f\"Accuracy: {best_model_result['accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_model_result['precision']:.4f}\")\n",
    "print(f\"Recall: {best_model_result['recall']:.4f}\")\n",
    "print(f\"F1-Score: {best_model_result['f1']:.4f}\")\n",
    "print(f\"Training Time: {best_model_result['training_time']:.2f}s\")\n",
    "\n",
    "# Per-class performance for best model\n",
    "print(f\"\\n=== PER-CLASS PERFORMANCE FOR {best_model_name} ===\")\n",
    "y_pred_best = best_model_result['predictions']\n",
    "\n",
    "for i, genre in enumerate(selected_genre_names):\n",
    "    precision = precision_score(y_test[:, i], y_pred_best[:, i], zero_division=0)\n",
    "    recall = recall_score(y_test[:, i], y_pred_best[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_test[:, i], y_pred_best[:, i], zero_division=0)\n",
    "    \n",
    "    print(f\"{genre:20s}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# Speed vs Accuracy analysis\n",
    "print(f\"\\n=== SPEED vs ACCURACY ANALYSIS ===\")\n",
    "print(\"Fastest models (by training time):\")\n",
    "fastest_models = performance_df.nsmallest(3, 'Training Time (s)')\n",
    "for _, row in fastest_models.iterrows():\n",
    "    print(f\"  {row['Model']:20s}: {row['Training Time (s)']:6.2f}s, F1={row['F1-Score']:.4f}\")\n",
    "\n",
    "print(\"\\nMost accurate models (by F1-Score):\")\n",
    "most_accurate = performance_df.nlargest(3, 'F1-Score')\n",
    "for _, row in most_accurate.iterrows():\n",
    "    print(f\"  {row['Model']:20s}: F1={row['F1-Score']:.4f}, Time={row['Training Time (s)']:6.2f}s\")\n",
    "\n",
    "# Efficiency analysis (F1-Score per second)\n",
    "performance_df['Efficiency'] = performance_df['F1-Score'] / performance_df['Training Time (s)']\n",
    "print(\"\\nMost efficient models (F1-Score per second):\")\n",
    "efficient_models = performance_df.nlargest(3, 'Efficiency')\n",
    "for _, row in efficient_models.iterrows():\n",
    "    print(f\"  {row['Model']:20s}: {row['Efficiency']:.6f} F1/s\")\n",
    "\n",
    "# Model recommendations\n",
    "print(f\"\\n=== MODEL RECOMMENDATIONS ===\")\n",
    "print(\"For maximum accuracy: {}\".format(most_accurate.iloc[0]['Model']))\n",
    "print(\"For fastest training: {}\".format(fastest_models.iloc[0]['Model']))\n",
    "print(\"For best efficiency: {}\".format(efficient_models.iloc[0]['Model']))\n",
    "print(\"For balanced performance: {}\".format(performance_df.iloc[0]['Model']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusions"
   },
   "source": [
    "## 9. Conclusions and Recommendations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Performance**: The analysis shows varying performance across different model types, with transformer models generally achieving higher accuracy due to their ability to capture complex linguistic patterns.\n",
    "\n",
    "2. **Speed vs Accuracy Trade-off**: Classic ML models are fastest to train but may sacrifice some accuracy, while transformer models provide the best accuracy but require more computational resources.\n",
    "\n",
    "3. **Feature Representation**: PCA visualization reveals how different feature extraction methods (TF-IDF, neural networks, transformers) represent the data in different dimensional spaces.\n",
    "\n",
    "4. **Multilabel Classification**: The confusion matrices show how well each model performs on individual genre classification tasks.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- **For Production**: Use the best performing transformer model for maximum accuracy\n",
    "- **For Real-time Applications**: Consider classic ML models for faster inference\n",
    "- **For Balanced Performance**: Choose the model with the best efficiency score\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "1. **Data Augmentation**: Increase training data through text augmentation techniques\n",
    "2. **Ensemble Methods**: Combine multiple models for improved performance\n",
    "3. **Hyperparameter Tuning**: Optimize model parameters for better results\n",
    "4. **Feature Engineering**: Explore additional text features beyond TF-IDF\n",
    "5. **Model Compression**: Use techniques like quantization for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# Save results for future reference\n",
    "results_summary = {\n",
    "    'dataset_info': {\n",
    "        'total_books': len(english_data),\n",
    "        'filtered_books': len(filtered_data),\n",
    "        'selected_genres': selected_genre_names,\n",
    "        'num_classes': len(selected_genre_names)\n",
    "    },\n",
    "    'performance_results': performance_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'metrics': {\n",
    "            'accuracy': best_model_result['accuracy'],\n",
    "            'precision': best_model_result['precision'],\n",
    "            'recall': best_model_result['recall'],\n",
    "            'f1_score': best_model_result['f1'],\n",
    "            'training_time': best_model_result['training_time']\n",
    "        }\n",
    "    },\n",
    "    'pca_results': {\n",
    "        'tfidf_variance': pca_results['TF-IDF']['explained_variance_ratio'].tolist(),\n",
    "        'neural_variance': pca_results['Neural Network']['explained_variance_ratio'].tolist(),\n",
    "        'transformer_variance': pca_results['Transformer']['explained_variance_ratio'].tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('genre_classification_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to 'genre_classification_results.json'\")\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(f\"Total models evaluated: {len(all_results)}\")\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {performance_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}